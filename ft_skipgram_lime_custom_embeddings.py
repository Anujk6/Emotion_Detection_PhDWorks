# -*- coding: utf-8 -*-
"""FT_Skipgram_LIME_CUSTOM_EMBEDDINGS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M2YW7szO3EhDZBV8GZxoJFbo2RfwVnTz
"""

from google.colab import drive
drive.mount('/content/drive')

# Step 2: Install Required Libraries
!pip install fasttext
!pip install indic-nlp-library
!pip install lime
!pip install tensorflow keras

import pandas as pd
import numpy as np
import re
from tqdm import tqdm
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Dropout
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
import fasttext
import os
import sys
from lime.lime_text import LimeTextExplainer
from keras.utils import to_categorical
from keras.models import load_model

INDIC_NLP_LIB_HOME = '/content/indic_nlp_library'
INDIC_NLP_RESOURCES = '/content/indic_nlp_resources'

!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git $INDIC_NLP_LIB_HOME
!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git $INDIC_NLP_RESOURCES

sys.path.append(INDIC_NLP_LIB_HOME)
from indicnlp.tokenize import indic_tokenize

train_df = pd.read_csv("New_Maltrain_data.csv",sep='\t')
val_df = pd.read_csv("New_Malval_data.csv",sep='\t')
test_df = pd.read_csv("New_Maltest_data.csv",sep='\t')

# Step 6: Word-level Tokenization
def tokenize_text(text):
    return indic_tokenize.trivial_tokenize(text, lang='ml')

def apply_tokenization(df):
    return [' '.join(tokenize_text(text)) for text in tqdm(df['text'].astype(str))]

train_df['tokens'] = apply_tokenization(train_df)
val_df['tokens'] = apply_tokenization(val_df)
test_df['tokens'] = apply_tokenization(test_df)

# Save tokenized version if needed
train_df.to_csv("/content/train_tok.csv", index=False)
val_df.to_csv("/content/val_tok.csv", index=False)
test_df.to_csv("/content/test_tok.csv", index=False)

# Step 7: Train FastText Model
with open("train_tok.txt", "w") as f:
    for line in train_df['tokens']:
        f.write(line + "\n")

ft_model = fasttext.train_unsupervised('train_tok.txt', model='skipgram', dim=100)
ft_model.save_model('/content/drive/MyDrive/fasttext_malayalam_skip.bin')

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_df['tokens'])

def get_embedding_matrix(tokenizer, ft_model, embed_dim=100):
    vocab_size = len(tokenizer.word_index) + 1
    embedding_matrix = np.zeros((vocab_size, embed_dim))
    for word, idx in tokenizer.word_index.items():
        embedding_matrix[idx] = ft_model.get_word_vector(word)
    return embedding_matrix

embedding_matrix = get_embedding_matrix(tokenizer, ft_model)

max_len = 50

X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['tokens']), maxlen=max_len)
X_val = pad_sequences(tokenizer.texts_to_sequences(val_df['tokens']), maxlen=max_len)
X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['tokens']), maxlen=max_len)

label_encoder = LabelEncoder()
y_train = to_categorical(label_encoder.fit_transform(train_df['category']))
y_val = to_categorical(label_encoder.transform(val_df['category']))
y_test = to_categorical(label_encoder.transform(test_df['category']))

model = Sequential()
model.add(Embedding(input_dim=embedding_matrix.shape[0],
                    output_dim=embedding_matrix.shape[1],
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))
model.add(LSTM(128, return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(y_train.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=32)

# Save the LSTM model
model.save("/content/drive/MyDrive/LSTM_ED_MALAYALAM_FT_SKIP.h5")

from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# Predict on test data
y_pred_probs = model.predict(X_test)
y_pred_classes = np.argmax(y_pred_probs, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Get class labels
labels = label_encoder.classes_

# Print full classification report
print("Classification Report:")
print(classification_report(y_true_classes, y_pred_classes, target_names=labels))

# Print overall scores
acc = accuracy_score(y_true_classes, y_pred_classes)
prec = precision_score(y_true_classes, y_pred_classes, average='weighted')
rec = recall_score(y_true_classes, y_pred_classes, average='weighted')
f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')

print(f"\n‚úÖ Test Accuracy: {acc:.4f}")
print(f"‚úÖ Precision (weighted): {prec:.4f}")
print(f"‚úÖ Recall (weighted): {rec:.4f}")
print(f"‚úÖ F1-score (weighted): {f1:.4f}")

from lime.lime_text import LimeTextExplainer
from tqdm import tqdm
import numpy as np
import pandas as pd

# Make sure you have these ready:
# - tokenizer
# - model (loaded LSTM)
# - label_encoder
# - class_names

# ‚úÖ Step 1: Define predict function
def predict_fn(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    padded = pad_sequences(sequences, maxlen=max_len)
    return model.predict(padded)

# Define class_names using the label_encoder
class_names = label_encoder.classes_

# ‚úÖ Step 2: Initialize LIME
explainer = LimeTextExplainer(class_names=class_names,split_expression=r'\s+')

# ‚úÖ Step 3: Regenerate explanations using word-level tokenized input
lime_results = []

for i in tqdm(range(len(test_df))):
    # Use space-separated tokenized sentence
    tokenized_text = test_df.iloc[i]['tokens']
    original_text = test_df.iloc[i]['text']

    # Generate explanation
    exp = explainer.explain_instance(tokenized_text, predict_fn, num_features=10)
    predicted_label = class_names[np.argmax(predict_fn([tokenized_text]))]

    lime_results.append({
        'Text': tokenized_text,
        'predicted_label': predicted_label,
        'explanation': exp.as_list()
    })

    # Optional: Print every 100th result
    if i % 100 == 0:
        print(f"\n‚úÖ Index {i}")

        print("Text:", tokenized_text)
        print("Predicted:", predicted_label)
        print("LIME Explanation:", exp.as_list())

# Convert list of tuples to string for CSV compatibility
lime_df = pd.DataFrame(lime_results)
lime_df['explanation'] = lime_df['explanation'].apply(lambda x: str(x))

# Save to Drive
lime_df.to_csv("/content/drive/MyDrive/lime_explanations_word_level_updated.csv", index=False)
print("‚úÖ LIME word-level explanations saved successfully.")

#print(f"‚úÖ LIME explanations collected so far: {len(lime_results)}")

# View the last explanation
#if lime_results:
    #last = lime_results[-1]
    #print("üîç Last Explanation:")
    #print("Text:", last['text'])
    #print("Predicted Label:", last['predicted_label'])
    #print("Explanation:", last['explanation'])
    #lime_df = pd.DataFrame(lime_results)
#lime_df.to_csv("/content/drive/MyDrive/lime_partial_results_wordlevel_skipgram.csv", index=False)
#print("üíæ Partial results saved.")

import pandas as pd

# Load the CSV
#lime_df = pd.read_csv("/content/drive/MyDrive/lime_partial_results_wordlevel_skipgram.csv")

# Show the first few rows
#print(lime_df.head(50))