# -*- coding: utf-8 -*-
"""MURilBased_Emotion_Crossvalidation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pq9Nru-59TEJA34_mReZJCIo9mDzjwB0

Importing Libraries
"""

!jupyter nbconvert --clear-output --inplace MURilBased_Emotion_Crossvalidation.ipynb

!pip install scikit-plot

!pip install transformers==4.31.0

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from simpletransformers.classification import ClassificationModel, ClassificationArgs
import sklearn
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score, accuracy_score
#import scikitplot as skplt
import matplotlib.pyplot as plt

#df = pd.read_csv("Kaggle_Mal_train.csv",sep='\t')
#df_eval = pd.read_csv("Kaggle_Mal_val.csv",sep='\t')
#df_test = pd.read_csv("Kaggle_Mal_test.csv",sep='\t')

df = pd.read_csv("New_Maltrain_data.csv",sep='\t')
df_eval = pd.read_csv("New_Malval_data.csv",sep='\t')
df_test = pd.read_csv("New_Maltest_data.csv",sep='\t')

df_eval.rename(columns={0:'text',1:'category'},inplace=True)
df_eval = df_eval[['text','category']]
df.rename(columns={0:'text',1:'category'},inplace=True)
df = df[['text','category']]
df_test.rename(columns={0:'text',1:'category'},inplace=True)
df_test = df_test[['text','category']]

num_labels = len(df['category'].unique())
keys = list(df['category'].unique())
values = list(range(0, num_labels))
label_dict = dict(zip(keys,values))
df['category'] = df['category'].apply(lambda x:label_dict[x])
df_eval['category'] = df_eval['category'].apply(lambda x:label_dict[x])
df_test['category'] = df_test['category'].apply(lambda x:label_dict[x])
num_labels

df

"""Balancing the imbalanced dataset"""

oversampling_count = 0

from collections import defaultdict

def oversample(df):
    classes = df['category'].value_counts().to_dict()
    max_count = max(classes.values())
    classes_dict = defaultdict(list)

    # Group data by category
    for index, row in df.iterrows():
        classes_dict[row['category']].append(row['text'])

    oversampled = []

    # Perform oversampling for each class
    for category, texts in classes_dict.items():
        if len(texts) < max_count:
            # Oversample if the class has fewer samples than the maximum count
            oversampled_texts = df[df['category'] == category].sample(n=max_count - len(texts), replace=True)
            oversampled.append(oversampled_texts)

    # Concatenate oversampled data with original DataFrame
    if oversampled:
        df = pd.concat([df] + oversampled, ignore_index=True)

    return df

print("Oversampling function was called", oversampling_count, "times.")

df=oversample(df)

df

from collections import defaultdict
from sklearn.utils import resample

def over_under_sample(df):
    unq_labels = df['category'].unique()
    data_dict = defaultdict(list)

    # Group texts by category
    for category, text in zip(df['category'], df['text']):
        data_dict[category].append(text)

    # Calculate required length per category
    req_len = len(df) // len(unq_labels)

    new_texts = []
    new_labels = []

    # Iterate through each category in data_dict
    for category, texts in data_dict.items():
        # If category has more samples than required length, truncate
        if len(texts) > req_len:
            texts = texts[:req_len]

        # Oversample to match the length of the category with maximum samples
        if len(texts) < req_len:
            oversampled_texts = resample(texts, replace=True, n_samples=req_len - len(texts))
            texts.extend(oversampled_texts)

        new_texts.extend(texts)
        new_labels.extend([category] * len(texts))

    # Return a DataFrame with oversampled data
    return pd.DataFrame({'text': new_texts, 'category': new_labels})

df = over_under_sample(df)

df

"""Training the Model"""

model_args = ClassificationArgs()

model_args.overwrite_output_dir=True
model_args.eval_batch_size=8
model_args.train_batch_size=8
model_args.learning_rate=4e-5
model_args.num_train_epochs = 4

model = ClassificationModel(
    'bert',
    'google/muril-base-cased',
    num_labels=12,
    args=model_args,
    tokenizer_type="bert",
    tokenizer_name='google/muril-base-cased'
)

for i in range(0,3):
    !rm -rf /kaggle/working/outputs
    model.train_model(df,eval_data=df_eval,acc=sklearn.metrics.classification_report)
    result, model_outputs, preds_list = model.eval_model(df_test,acc=sklearn.metrics.classification_report)
    for j in result.values():
        print(j)

predictions, raw_outputs = model.predict(df_test['text'].to_list())

df_final = df_test.copy()
reverse_label_dict = {v:u for u,v in label_dict.items()}
reverse_label_dict
df_final['Predicted_Labels'] = predictions
df_final['Predicted_Labels'] = df_final['Predicted_Labels'].apply(lambda x:reverse_label_dict[x])
df_final['category'] = df_final['category'].apply(lambda x:reverse_label_dict[x])
df_final['pid'] = df_final.index
df_final = df_final[['pid','Predicted_Labels','category']]

df_final

score = f1_score(df_final['category'],df_final['Predicted_Labels'],average='macro')
print("The macro average f1 score is:" + str(score))

accuracy= accuracy_score(df_final['category'],df_final['Predicted_Labels'])
print("The accuracy is:" + str(accuracy))

import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

# Specify the number of folds
n_splits = 5  # You can adjust the number of folds as needed

# Initialize StratifiedKFold
stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

# Initialize a metric (e.g., accuracy) to keep track of performance across folds
overall_accuracy = 0.0

# Perform cross-validation
for fold, (train_index, val_index) in enumerate(stratified_kfold.split(df_eval['text'], df_eval['category'])):
    print(f"\nFold {fold + 1}:")

# Split the data into training and validation sets
    train_data, val_data = df_eval.iloc[train_index], df_eval.iloc[val_index]

    # Train the model
   # model.train_model(train_data)

    # Evaluate on the validation set
    predictions, raw_outputs  = model.predict(val_data['text'].to_list())
    accuracy = accuracy_score(val_data['category'], predictions)

    print(f"Accuracy: {accuracy:.4f}")

    # Aggregate performance metric
    overall_accuracy += accuracy

# Calculate average accuracy
accuracy_scores = [0.8342,  0.8472, 0.8445, 0.8606, 0.8552]
#average_accuracy = sum(accuracy_scores) / len(accuracy_scores)
average_accuracy = sum(accuracy_scores) / n_splits
print(f"Average Accuracy: {average_accuracy}")

# Assuming 'model' is your trained ClassificationModel
# Replace 'model' with your actual model variable name
model.save_model("/content/drive/My Drive/Emotion_Model_Directory")  # Save model

# Save tokenizer separately
model.tokenizer.save_pretrained("/content/drive/My Drive/Emotion_Model_Directory")  # Save tokenizer

import pandas as pd
import matplotlib.pyplot as plt

# Assuming df_final contains the DataFrame with 'Predicted_Labels' and 'category' columns

# Compute misclassifications
df_final['Misclassified'] = df_final['Predicted_Labels'] != df_final['category']

# Aggregate misclassification counts
error_counts = df_final.groupby(['category', 'Misclassified']).size().unstack().fillna(0)

# Plotting graph for error analysis
error_counts.plot(kind='bar', stacked=True, figsize=(5, 6))
plt.xlabel('Categories')
plt.ylabel('Count')
plt.title('Misclassification Count by Category with MuRIL model')
plt.legend(title='classification', labels=['Correct', 'Misclassified'])
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

